# Recurrent Neural Network (RNN)

This is the readme for the "RNN.py" file, created by George Barker and Andre Zeromski. This file is used to preprocess data and perform feedforward for a RNN language model. This was done from scratch without TensorFlow.

## How to run code for Feedforward

Run the python file RNN.py and the script will load in the tokenized and vectorized data from part 1 using pickle. The data consists of a list with each element being an array of a line in the original .csv which has been tokenized and each token, vectorized. The output to running the feedforward on this list will be an array with the word predictions for each word in a line. For simplicity sake we only show the predictions for the first 10 lines.

If you'd like to load in all data from scratch without using pickle, you can comment out lines 164 - 165 and 176 - 177 and uncomment lines 181-184. Then run the python file. This will take some time, but the data is loaded from the provided .csv file and the first 10 lines are feed-forwarded through our RNN.

## Feedforward

We begin this portion of our project by curating the dataset. We use the entire list of vectorized Reddit comments as our training set. We do this because the model currently has no back propagation method to actually train and update our model’s weights, thus having a split dataset would be obsolete. We will split our dataset 80/20 testing and training after we have implemented the backpropagation through time.

The model’s x_train and y_train were created by calling getNumericFromWord and getNumericFromSentence from the vectorizeData class. getNumericFromWord accepts a single word/token from the Reddit comment dataset and transforms and returns it as a numeric vector by calling the RNN’s trained word2vec model. getNumericFromSentence takes each line of the reddit comment, and for each word/token in that line, it calls getNumericFromWord and adds the returned vector to a list, where the list ultimately represents a whole comment/row.

Once the entire dataset has been transformed into a 3 dimensional tensor (the first dimension is an array of the vectorized token, the second is the list/comment containing each vector, and the third dimension contains each list/comment for every row of the input data), we create x_train and y_train. We begin by removing each ‘START’ and ‘END’ vector from the beginning and end of each line, and call this new tensor vectorLineCopy. x_train is created as a new tensor by popping the last vector of each line from vectorLineCopy, and y_train created by popping the first vector of each line in vectorLineCopy. We do this because each vector in x_train is supposed to predict the following vector in the comment, which is why y_train contains all vectorized tokens except for the first one of the line. x_train will be the input of our RNN, and y_train the desired output at each time step of the RNN. We pickle x_train in our main function to reduce computation time.

Our RNN model inits with the gensim Word2VecModel, and optional hidden_dim = 100 and bptt_truncate=4 as parameters. The input layer size is initialized based on the size of the vector for a word in our Word2VecModel. The Word2VecModel saved in pickle has a word vector of length 5. Thus the input layer is 5. The output layer for softmax size is obtained using the size of the vocabulary of the Word2VecModel. In this case, the size is 8003, including our 8000 most frequently occurring words, as well as our other added tokens.

After initializing our RNN, we iterate through our x_train data and call the feedforward method on each element of the x_train data. For testing purposes, we have limited iterations to the first 10 lines of reddit comments. The feedforward method takes in a list of vectorized tokens and outputs a list where each element is the model's prediction for the most likely token output for the given timestep.

Feedforward first initializes the vector “hp”, or the previous state of the hidden layer. This vector is a vector of size 5, or the size of our vectorized words, and consists of 0s. We initialize it as this value because for feedforward’s first time step, there is no previous state of the hidden layer. Next, the model initializes an empty list “to” that will contain the output for each time step. For each forward pass through time, the model sequentially inputs each vectorized word at each timestep, and uses each cell of the vector as a node for the input layer. The values from the input layer is then propagated forward to the hidden layer through the weight matrix U and stored in the matrix “hi”. The previous state of the hidden layer, “hp,” from the previous time step is also propagated forward through the weight matrix W. These two propagations from the input layer and previous hidden state are summed together in “hiddenSummation.” This is input to a tanh activation function and stored in ht. “ht” is used in the next time step as “hp”. These values compose the hidden layer. Finally, the hidden layer’s values are propagated forward through the weight matrix V to obtain the output at that time step. This output is activated by a softmax function. The model outputs to a softmax because it is giving a probability of the next token from the 8003 vocabulary of words. The max node is returned, which corresponds to a token in the vocabulary, and the vector representation of that token is appended to “to.” At the last timestep, “to” is returned.

## Data Manipulation

This assignment represents the data preprocessing segment of an RNN. The goal of this segment is to embed words from Reddit comments as floating point vectors. These vectors will later allow the model to apply linear and non-linear transformations on them to predict the next token (word) given the previous word embeddings as context. The model creates these embeddings by tokenizing each word in the input text, because tokens can be more easily and methodologically represented as numerical vectors (as opposed to leaving the input text as strings or one hot encodings). These tokenized words then make up rows of text that are also tokenized and passed to a word2vec method, which embeds these tokenized sentences and words with numerical values. The Word2vec encoding creates a vector space such that words appearing in similar contexts will appear nearby in vector space. It will be useful for our linguistic model to represent words in vector space as we want to capture the meaning of a word in our model.

We were provided code that initialize the dimensions for the RNN: word_dim is the size of the model’s input layer, hidden_dim is the size of the model’s hidden layer, bptt_truncate is the number of times the model recurs for BPTT, and U,V, and W are the weight matrices. These variables will not be called in this segment. The model’s first called method is importData, which accepts a .csv file as input and returns the text within it for manipulation. The text is stored in an array with each element of the array as a line of the .csv.

Next, the model calls tokenizeLines, which accepts rows of text (arrayOfLines) as input. tokenizeLines begins by separating the input text row by row and calls the method tokenizeText on each row. The method tokenizeText takes these rows of text and tokenizes each word and symbol according to NLTK's default word_tokenize method. We want to tokenize these words because tokenizing divides large chunks of texts into more manageable, smaller pieces that can be manipulated into numerical vectors through embedding, which will be useful for finding patterns within the text. Once all words and symbols have been tokenized, we change them all to lowercase and remove stop words (using NLTK's list of stop words), because stop words are typically non-informative of the meaning of the sentence or string of text. It is important to note that changing words to lowercase means we can consider words the same even when they are capitilized at the beginning of a sentence and we can add our own special tokens such as "END". This does however remove meaning from some words. For example, Apple and apple are not the same (company vs food). The goal with these steps are to normalize our input data.

Once each row in tokenizeLine's arrayOfLines has been normalized in this way and returned as a 2D vector (1 column by the number of rows of text from the input file), tokenizeLines proceeds to flatten this vector and calls findVocab. findVocab is a method that takes this tokenized flattened vector and returns the 8000 most frequently occurring words. These 8000 words are then used as our new vocabulary; we want to limit our vocabulary for two reasons. The first is that, according to Zipf's law, we can limit our vocabulary in this way and still retain the same or similar meaning in each sentence. The second reason is if our model's prediction of words is based on the probability of each word occurring next, then a large vocabulary will cause these probabilities to be very small, and thus it is difficult to determine a useful understanding of an output probability. Once  the newly developed vocabulary is created, the model proceeds to take the unflattened arrayOfLines and replaces the words not in the vocabulary with an 'UNK' token.

Each row in arrayOfLines is then manipulated by adding a 'START' and 'END' token to the beginning and end of each line. We do this because it signals to the encoder--decoder structure of the embedding model when to begin encoding and when to terminate to create the final embedding of the given input sequence. Finally, the model calls wordToVec, which creates a Word2Vec model that is saved after training on these tokenized rows of sentences from arrayOfLines. The Word2Vec model contains a total of 8003 words in its vocabulary (8000 most frequent words, "UNK", "START", and "END") and is what is used to embed our tokenized sentences with numerical meaning.

### Sources

https://machinelearningmastery.com/clean-text-machine-learning-python/ <br />
https://machinelearningmastery.com/develop-word-embeddings-python-gensim/ <br />
https://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.XcJOey_MyYV <br />
